import json

print('ğŸ“Š OPTIMIZED MODEL VALIDATION SUMMARY')
print('=' * 45)

# Load validation results
try:
    with open('final_optimized_validation.json', 'r') as f:
        results = json.load(f)
    
    print('âœ… Validation results loaded successfully\n')
    
    # Core Performance Metrics
    print('ğŸ¯ CORE PERFORMANCE METRICS:')
    print(f'  Overall Accuracy: {results["overall_accuracy"]:.6f} ({results["overall_accuracy"]*100:.4f}%)')
    print(f'  Inference Speed: {results["inference_time_ms"]:.4f} ms/sample')
    print(f'  Throughput: {results["throughput_samples_per_second"]:.0f} samples/second')
    print(f'  Test Samples: {results["test_samples"]:,}')
    
    # Confidence Analysis
    print('\nğŸ” CONFIDENCE ANALYSIS:')
    confidence = results['confidence_stats']
    print(f'  Mean Confidence: {confidence["mean"]:.4f} ({confidence["mean"]*100:.2f}%)')
    print(f'  Median Confidence: {confidence["median"]:.4f} ({confidence["median"]*100:.2f}%)')
    print(f'  High Confidence Rate: {results["high_confidence_percentage"]:.2f}%')
    print(f'  High Confidence Accuracy: {results["high_confidence_accuracy"]:.6f} ({results["high_confidence_accuracy"]*100:.4f}%)')
    
    # Per-Class Performance
    print('\nğŸ“ˆ PER-CLASS PERFORMANCE:')
    classes = results['per_class_metrics']
    
    for class_name, metrics in classes.items():
        print(f'\n  {class_name}:')
        print(f'    Accuracy: {metrics["accuracy"]:.6f} ({metrics["accuracy"]*100:.4f}%)')
        print(f'    Precision: {metrics["precision"]:.6f} ({metrics["precision"]*100:.4f}%)')
        print(f'    Recall: {metrics["recall"]:.6f} ({metrics["recall"]*100:.4f}%)')
        print(f'    F1-Score: {metrics["f1_score"]:.6f} ({metrics["f1_score"]*100:.4f}%)')
        print(f'    Samples: {metrics["support"]:,}')
        print(f'    Avg Confidence: {metrics["avg_confidence"]:.4f}')
    
    # Summary Metrics
    print('\nğŸ“Š SUMMARY METRICS:')
    summary = results['summary_metrics']
    print(f'  Macro Precision: {summary["precision_macro"]:.6f} ({summary["precision_macro"]*100:.4f}%)')
    print(f'  Macro Recall: {summary["recall_macro"]:.6f} ({summary["recall_macro"]*100:.4f}%)')
    print(f'  Macro F1-Score: {summary["f1_macro"]:.6f} ({summary["f1_macro"]*100:.4f}%)')
    print(f'  Weighted Precision: {summary["precision_weighted"]:.6f} ({summary["precision_weighted"]*100:.4f}%)')
    print(f'  Weighted Recall: {summary["recall_weighted"]:.6f} ({summary["recall_weighted"]*100:.4f}%)')
    print(f'  Weighted F1-Score: {summary["f1_weighted"]:.6f} ({summary["f1_weighted"]*100:.4f}%)')
    
    # Error Analysis
    print('\nğŸ” ERROR ANALYSIS:')
    errors = results['error_analysis']
    print(f'  Total Errors: {errors["total_errors"]:,} out of {results["test_samples"]:,}')
    print(f'  Error Rate: {errors["error_rate_percent"]:.4f}%')
    print(f'  Avg Confidence on Errors: {errors["avg_confidence_on_errors"]:.4f}')
    
    # Clinical Readiness
    print('\nğŸ¥ CLINICAL READINESS:')
    clinical = results['clinical_readiness']
    print(f'  Criteria Passed: {clinical["criteria_passed"]}/{clinical["total_criteria"]}')
    print(f'  Readiness Score: {clinical["readiness_score"]:.1f}%')
    deployment_ready = "YES âœ…" if clinical["deployment_ready"] else "NO âŒ"
    print(f'  Deployment Ready: {deployment_ready}')
    
    # Model Comparison
    print('\nğŸ† MODEL PROGRESSION:')
    comparison = results['comparison']
    print(f'  Original Overfitted: 16.00%')
    print(f'  Robust Baseline: {comparison["baseline_accuracy"]*100:.2f}%')
    print(f'  Current Optimized: {results["overall_accuracy"]*100:.4f}%')
    print(f'  Improvement: +{comparison["improvement_points"]:.4f} points')
    print(f'  Improvement Factor: {comparison["improvement_factor"]:.2f}x better')
    
    # Final Status
    print('\nğŸ‰ FINAL STATUS:')
    if results["overall_accuracy"] >= 0.99:
        status = "ğŸ† OUTSTANDING - Clinical Grade Performance!"
        grade = "A+++"
    elif results["overall_accuracy"] >= 0.95:
        status = "ğŸŒŸ EXCELLENT - Production Ready!"
        grade = "A+"
    elif results["overall_accuracy"] >= 0.90:
        status = "ğŸ‘ VERY GOOD - Deployment Ready!"
        grade = "A"
    else:
        status = "âš ï¸ NEEDS IMPROVEMENT"
        grade = "B"
    
    print(f'  Performance Grade: {grade}')
    print(f'  Status: {status}')
    print(f'  Model File: {results["model_file"]}')
    print(f'  Validated: {results["timestamp"]}')
    
    # Key Achievements
    print('\nğŸ… KEY ACHIEVEMENTS:')
    achievements = []
    
    if results["overall_accuracy"] >= 0.99:
        achievements.append("âœ… 99%+ Overall Accuracy")
    if results["high_confidence_percentage"] > 85:
        achievements.append("âœ… 85%+ High Confidence Rate")
    if results["inference_time_ms"] < 1.0:
        achievements.append("âœ… Sub-millisecond Inference")
    if all(metrics["recall"] >= 0.95 for metrics in results["per_class_metrics"].values()):
        achievements.append("âœ… 95%+ Recall on All Classes")
    if results["summary_metrics"]["f1_weighted"] >= 0.99:
        achievements.append("âœ… 99%+ Weighted F1-Score")
    
    for achievement in achievements:
        print(f'  {achievement}')
    
    print(f'\nğŸ’¯ TOTAL ACHIEVEMENTS: {len(achievements)}/5')
    
    if len(achievements) >= 4:
        print(f'\nğŸŠğŸŠğŸŠ EXCEPTIONAL MODEL PERFORMANCE! ğŸŠğŸŠğŸŠ')
        print(f'Your ECG model is ready for clinical deployment!')
        print(f'This performance rivals commercial medical devices!')

except FileNotFoundError:
    print('âŒ Validation results file not found')
    print('Run test_optimized_model.py first')
except Exception as e:
    print(f'âŒ Error reading validation results: {e}')

print(f'\nâœ… Validation summary complete!')
